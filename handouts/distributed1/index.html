Last Updated on 2022/04/06     

<?xml version="1.0" encoding="iso-8859-1" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">  
<!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->  
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head> <title>Distributed Systems I</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<!-- fn-in,html,htex4ht,xhtml --> 
<meta name="src" content="main.tex" /> 
<link rel="stylesheet" type="text/css" href="main.css" /> 
<link rel="stylesheet" href="https://latex.now.sh/style.css"> 
<link rel="stylesheet" href="/notes.css"> 
<style>body {max-width: 100ch;} 
dl dd {text-align: left}</style> 
</head><body 
>
   <div class="maketitle"><a 
 id="Q1-1-1"></a>
_________________________________________________________________________________________________
<h2 class="titleHead">Distributed Systems I</h2>                                                                   Software Architecture
   <div class="date" >March 20, 2023</div>                                                                                                                       <div class="author" >Brae Webb &amp; Richard
Thomas</div>
____________________________________________________________________________________________
   </div>
   <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-10001"></a>Introduction</h3>
<!--l. 8--><p class="noindent" >We have started looking at distributing our applications. Service-based architectures distribute business
processes such that each process is deployed on a separate physical machine <span class="cite">[<a 
href="#Xservice-based-notes">1</a>]</span>. The distributed
machines act as if they are the one system, so service-based architectures are our first look at distributed
systems.
<a 
 id="x1-1001r1"></a>
</p>
   <div class="tcolorbox tcolorbox" id="tcolobox-1">    
<div class="tcolorbox-title">
   </div> 
<div class="tcolorbox-content"><!--l. 15--><p class="noindent" ><span 
class="Cabin-Bold-tlf-t1-x-x-120">Definition</span><span 
class="Cabin-Bold-tlf-t1-x-x-120">&#x00A0;1. Distributed System</span>
</p><!--l. 16--><p class="noindent" >A system with multiple components located on <span 
class="Cabin-Italic-tlf-t1-x-x-120">different machines </span>that communicate and coordinate
actions in order to <span 
class="Cabin-Italic-tlf-t1-x-x-120">appear as a single coherent system </span>to the end-user. </p> 
</div> 
</div>
<!--l. 19--><p class="noindent" >Recall that during our investigation of service-based architectures we outlined the following pros and
cons:
      </p><dl class="description"><dt class="description">
<span 
class="Cabin-Bold-tlf-t1-x-x-120">Simplicity</span> </dt><dd 
class="description">
      <!--l. 21--><p class="noindent" >For a distributed system.
      </p></dd><dt class="description">
<span 
class="Cabin-Bold-tlf-t1-x-x-120">Reliability</span> </dt><dd 
class="description">
      <!--l. 23--><p class="noindent" >Independent services spreads the risk of fall-over.
      </p></dd><dt class="description">
<span 
class="Cabin-Bold-tlf-t1-x-x-120">Scalability</span> </dt><dd 
class="description">
      <!--l. 24--><p class="noindent" >Course-grained services.</p></dd></dl>
<!--l. 27--><p class="noindent" >Let us take a moment now to reflect deeper on each of these attributes.
</p><!--l. 29--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-20002"></a>Simplicity</h3>
<!--l. 30--><p class="noindent" >We said that a service-based architecture was simple &#8216;for a distributed system&#8217;. This stipulation is doing a lot of
heavy lifting. It can be easy to forget given the wide-spread usage and demand for distributed systems that they
are not simple. Let us investigate some logical fallacies that make distributed systems more complicated and less
appealing than they might initially appear.
<a 
 id="x1-2001r2"></a>
                                                                                                    
                                                                                                    
</p>
   <div class="tcolorbox tcolorbox" id="tcolobox-2">    
<div class="tcolorbox-title">
   </div> 
<div class="tcolorbox-content"><!--l. 43--><p class="noindent" ><span 
class="Cabin-Bold-tlf-t1-x-x-120">Definition</span><span 
class="Cabin-Bold-tlf-t1-x-x-120">&#x00A0;2. Fallacy</span>
</p><!--l. 44--><p class="noindent" >Something that is believed or assumed to be true but is not. </p> 
</div> 
</div>
<!--l. 47--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-30003"></a>Fallacies of Distributed Computing</h3>
<!--l. 48--><p class="noindent" >The first four fallacies of distributed computing were introduced by Bill Joy and Dave Lyon and called &#8216;The Fallacies
of Networked Computing&#8217; <span class="cite">[<a 
href="#Xfour-fallacies">2</a>]</span>. Peter Deutsh introduced three more fallacies and named the collection &#8216;The
Fallacies of Distributed Computing&#8217; <span class="cite">[<a 
href="#Xeight-fallacies">3</a>]</span>. James Gosling (lead developer of Java) later introduced a final eighth
fallacy <span class="cite">[<a 
href="#Xfour-fallacies">2</a>]</span>.
</p><!--l. 52--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a 
 id="x1-40003.1"></a>The Network is Reliable</h4>
<!--l. 53--><p class="noindent" >The first fallacy assures us that the network is reliable. Developers like to believe that this is a fact which can be
relied upon but networks are certainly not reliable. They are more reliable than they used to be but they are far
from reliable.
</p><!--l. 57--><p class="indent" >   Often we are quite capable of remembering this requirement in obviously unreliable contexts. For example,
building a mobile application, it is easy enough to remember that the mobile will occasionally lose network access
and we should plan for that. Developers can often get the client-server connection correct but neglect
server-server connections, assuming they are reliable.
</p><!--l. 63--><p class="indent" >   When building distributed systems which need to communicate with each other, we often assume that the
connection between servers will always be reliable. However, even in the most tightly controlled network we
cannot guarantee that no packets will be dropped. It is important to remember that the reliability of network
communication is a fallacy and that we need to build-in error handling between any two machines.
We will see later in our distributed systems series that processing network errors can be extremely
challenging.<span class="footnote-mark"><a 
href="#fn1x0" id="fn1x0-bk"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-4001f1"></a>
</p><!--l. 69--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a 
 id="x1-50003.2"></a>Latency is Zero</h4>
<!--l. 70--><p class="noindent" >This course is being run in Australia, no one who is taking this course in Australia <span 
class="Cabin-Italic-tlf-t1-x-x-120">should </span>believe this fallacy. Again,
this fallacy does not exist as much with client-server communication, we are all intimately familiar with having
videos buffer.
</p><!--l. 75--><p class="indent" >   This fallacy still lingers with server-server communication. Each time an architect makes a decision to
distribute components onto different machines, they make a trade-off, they are making a non-trivial performance
sacrifice. If you are designing a system with distributed calls, ensure that you know the performance characteristics
of the network and deem the performance trade-offs acceptable.
</p><!--l. 82--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a 
 id="x1-60003.3"></a>Bandwidth is Infinite</h4>
                                                                                                    
                                                                                                    
<!--l. 83--><p class="noindent" >Similar to the previous fallacy, the fallacy of infinite bandwidth is a plea for architects to be mindful and
conservative in their system designs. We need to be mindful of the internal and external consumption
of bandwidth for our systems. There are hard limits on bandwidth. A dubious statement from
Wikipedia<span class="footnote-mark"><a 
href="#fn2x0" id="fn2x0-bk"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-6001f2"></a>
claims that Australia has a total trans-Pacific bandwidth of around 704 GB/s.
</p><!--l. 91--><p class="indent" >   Internally, data centres such as AWS allow impressive bandwidths and it is therefore becoming less of a
problem. However, when working at scale, it is important to be mindful to not be wasteful with the size of data
communicated internally. Externally, bandwidth usage comes at the cost of the end-user and the budget.
End-users suffer when bandwidth usage of an application is high, not all users have access to high bandwidth
connections, particularly those in developing nations. There is also a cost on the developers end, as infrastructure
providers charge for external bandwidth usage.
</p><!--l. 99--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.4    </span> <a 
 id="x1-70003.4"></a>The Network is Secure</h4>
<!--l. 100--><p class="noindent" >Developers occasionally assume that VPCs, firewalls, security groups, etc. ensure security. This is not the case. The
moment a machine is connected to the internet it is vulnerable to a whole range of attacks. Known and unknown
vulnerabilities enable access, bypassing network security infrastructure. Injection attacks are still prominent; if a
distributed system does not have multiple authentication checkpoints then an injection attack on one insecure
machine can compromise the entire system. We tend to incorrectly assume we can trust the machines within our
network, and this trust is a fallacy.
</p><!--l. 109--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.5    </span> <a 
 id="x1-80003.5"></a>The Topology Never Changes</h4>
<!--l. 110--><p class="noindent" >The topology of a network encompasses all of the network communication devices, including server
instances. This includes routers, hubs, switches, firewalls, etc. Fortunately, in modern cloud computing we
have more control over these network devices. Unfortunately, this control also gives us the ability to
rapidly change the topology. The churn of network topologies in the cloud age makes us less likely to
make previously common mistakes like relying on static IP addresses. However, we still need to be
mindful of network changes. If network topology changes in such a way that latency is increased
between machines then we might end up triggering application timeouts and rendering our application
useless.
</p><!--l. 118--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.6    </span> <a 
 id="x1-90003.6"></a>There is Only One Administrator</h4>
<!--l. 119--><p class="noindent" >This is a fallacy which is encountered infrequently but it is worth pointing out. Imagine you have an
application deployed on AWS. To prevent an overly eager graduate developer from taking down your
application, your build pipeline does not run on the weekend. Sunday afternoon you start getting
bug reports. Users online are unhappy. Your manager is unhappy. You check the logs, there have
been no new deployments since Friday. Worse still you can access the application and it works fine.
Who do you contact? AWS? The users? Your ISP? Your users ISP&#8217;s? Who is the mythical sysadmin
to solve all your problems? There isn&#8217;t one; it is important to account for and plan for that. When
things start failing, can you deploy to a different AWS region? Can you deploy to a different hosting
provider? Can you deploy parts of your application on-premise? Likewise we need to be aware of
                                                                                                    
                                                                                                    
this fallacy when trying to resolve less drastic failures, for example, high latency, permission errors,
etc.
</p><!--l. 138--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.7    </span> <a 
 id="x1-100003.7"></a>Transport Cost is Zero</h4>
<!--l. 139--><p class="noindent" >When architecting a system it can be easy to get carried away with building beautiful distributed, modular,
extensible systems. However, we need to be mindful that this costs money. Distributed systems cost far
more than monolithic applications. Each RPC or REST request translates to costs. We should also
mention that under-utilised machines cost money. Distributing our application services can seem like a
beautiful deployment solution but if you are running 10 different service machines for an application
with 100 users, you are burning money both hosting the machines and communicating between
them.
</p><!--l. 148--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">3.8    </span> <a 
 id="x1-110003.8"></a>The Network is Homogeneous</h4>
<!--l. 149--><p class="noindent" >This fallacy has gotten both better and worse over time. The network has become even more heterogeneous over
time. There are more types of networks over which most systems need to operate. There are more
communication protocols to consider when designing a distributed system. But, a judicious selection of a small set
of standard protocols can hide this complexity.
</p><!--l. 155--><p class="indent" >   A simple example of this is accessing a system from a desktop computer with a wired connection to a
router, from a laptop over WiFi, and from a phone over 5G, This system is obviously operating over a
heterogeneous network, but if you have designed the system using standard communication protocols, the
complexity of its heterogeneous network is hidden behind the layer of abstraction provided by the
protocols.
</p><!--l. 162--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-120004"></a>Reliability</h3>
<!--l. 163--><p class="noindent" >We said previously that a service-based distributed architecture was reasonably reliable as it had independent
services which spreads the risk of fall-over. We should look a bit more into why we need reliable software, what
reliable software is, how we have traditionally achieved reliable software, and how we can use distributed systems
to create more reliable software.
</p><!--l. 169--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">4.1    </span> <a 
 id="x1-130004.1"></a>Reliable Software</h4>
<!--l. 170--><p class="noindent" >We want, and in some cases, need, our software to be <span 
class="Cabin-Italic-tlf-t1-x-x-120">reliable</span>. Our motivation for reliable software ranges from
life or death situations all the way to financial motivations. At the extreme end we have radiation therapy
machines exposing patients to too much radiation and causing fatalities <span class="cite">[<a 
href="#Xtherac">4</a>]</span>. On the less extreme end, we have
outages from Facebook causing $60 million of lost revenue <span class="cite">[<a 
href="#Xfacebook-outage">5</a>]</span>. Regardless of the motivation, we need reliable
software. But what does it mean for software to be reliable?
                                                                                                    
                                                                                                    
</p><!--l. 179--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">4.2    </span> <a 
 id="x1-140004.2"></a>Fault Tolerance</h4>
<!--l. 180--><p class="noindent" >In an ideal world we would produce fault-proof software, where we define a fault as something
going wrong. Unfortunately, we live in an extremely non-ideal world. Faults are a major part of
our software world. Even if we could develop bug-free software, hardware would still fail on
us. If we could invent perfectly operating hardware, we would still be subject to <a 
href="https://www.youtube.com/watch?v=AaZ_RSt0KP8" >cosmic bit
flipping</a><span class="footnote-mark"><a 
href="#fn3x0" id="fn3x0-bk"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-14001f3"></a>.
</p><!--l. 189--><p class="indent" >   Instead, we learnt long ago that fault tolerance is the best way to develop reliable systems. John von Neumann
was one of the first to integrate the notion of fault tolerance to develop reliable hardware systems in
the 1950s <span class="cite">[<a 
href="#Xneumann-faults">6</a>]</span>. Fault tolerant systems are designed to be able to recover in the event of a fault. By
anticipating faults, rather than putting our heads in the sand, we can develop much more reliable
software.
</p><!--l. 195--><p class="indent" >   A part of this philosophy is to write defensive software which anticipates the <span 
class="Cabin-Italic-tlf-t1-x-x-120">likely </span>software logic
faults (bugs). The <span 
class="Cabin-Italic-tlf-t1-x-x-120">likely </span>modifier is important here, if we write paranoid code which has no trust of
other systems, our code will become incredibly complex and ironically more bug prone. Instead, use
heuristics and past experience to anticipate systems which are likely to fail and place guards around such
systems.
</p><!--l. 201--><p class="indent" >   Aside from software logic faults, we have catastrophic software faults and hardware faults. These types of
faults cause the software or hardware to become unusable. This occurs in practice far more often than you might
expect. Hard disks have a 10 to 50 year mean time to failure <span class="cite">[<a 
href="#Xdata-intensive">7</a>]</span>. We would only need 1,000 disks to have one die
every 10 days. How should we tolerate this type of fault?
</p><!--l. 209--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">4.3    </span> <a 
 id="x1-150004.3"></a>Distributing Risk</h4>
<!--l. 210--><p class="noindent" >For faults which cause a system to become unusable we cannot program around it. We need
a mechanism for recovering from the fault without relying on the system to work at all as
expected. One approach is to duplicate and distribute the original system. If we duplicate our
software across two machines then we have halved our risk of the system going completely
down.<span class="footnote-mark"><a 
href="#fn4x0" id="fn4x0-bk"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-15001f4"></a>
If we replicate our software across thousands of machines then the likelihood of a complete system
failure due to a hardware failure is negligible. Google does not go down over a hardware failure. We
know that Google servers have many hardware failures per day but this does not cause a system
outage.
</p><!--l. 219--><p class="indent" >   This gives us one very important motivation for creating distributed systems, to ensure that our software
system is <span 
class="Cabin-Italic-tlf-t1-x-x-120">reliable</span>.
</p><!--l. 233--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">5    </span> <a 
 id="x1-160005"></a>Scalability</h3>
<!--l. 234--><p class="noindent" >Thus far we have foreshadowed a number of the complexities inherit to distributed systems, we have
also reasoned that somewhat counter-intuitively distributing our system can offer us greater overall
reliability. Finally, let us shallowly explore the ways cloud platforms can offer us reliable systems via
replication.
                                                                                                    
                                                                                                    
</p><!--l. 238--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">5.1    </span> <a 
 id="x1-170005.1"></a>Auto-scaling</h4>
<!--l. 239--><p class="noindent" >Auto-scaling is a feature offered by many cloud platforms which allows dynamic provisioning of compute
instances. Kubernetes is a non-cloud platform specific tool which also offers auto-scaling. Auto-scaling is specified
by a scaling policy which a developer configures to specify when new instances are required, and when existing
instances are no longer required.
</p><!--l. 243--><p class="indent" >   Auto-scaling policies will specify a desired capacity which is how many instances should currently be running.
The actual capacity is how many instances are currently running, this is different from the desired capacity as
instances take time to spin up and down. The desired capacity can be based on metrics such as CPU usage,
network usage, disk space, etc. configured via the scaling policy. We should also specify minimum and maximum
capacity. Our minimum capacity prevents spinning down too many instances when load is low, for
example we do not want to have zero actual capacity as a user will need to wait for an instance to
spin up to use the service. Likewise the maximum capacity prevents over-spending when load is
high.
</p><!--l. 252--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">5.2    </span> <a 
 id="x1-180005.2"></a>Health Checks</h4>
<!--l. 254--><p class="noindent" >In addition to using auto-scaling to respond to dynamic load we can utilise health checks to ensure all
running instances are functional. Health checks allow us to build reliable software. A health check is a
user specified method for the auto-scaling to check if an instance is healthy. When a health check
identifies that an instance is not healthy, it will spin the instance down and replace it with a new healthy
instance.
</p><!--l. 259--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">5.3    </span> <a 
 id="x1-190005.3"></a>Load-balancing</h4>
<!--l. 261--><p class="noindent" >An auto-scaling group will allow us to automatically provision new instances based on specified metrics. In
combination with health checks we can keep a healthy pool of the correct amount of instances.
However, the question arises, how do we actually use these instances together? If we are accessing these
instances via network connections we can use a load-balancer. A load-balancer is placed in front of an
auto-scaling group. All traffic for a service is sent to the load-balancer and as the name implies, the
load-balancer will balance the traffic sent to instances within the auto-scaling group. This allows a group
of instances to assume the same end-point with the load-balancer forwarding traffic to individual
instances.
</p><!--l. 271--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">6    </span> <a 
 id="x1-200006"></a>Conclusion</h3>
<!--l. 272--><p class="noindent" >We have revisited three of the important quality attributes of a service-based architecture. Namely; simplicity,
reliability, and scalability. For simplicity we have seen that there are a number of challenges implicit with any
distributed system. For reliability we have seen how using a distributed system can increase the reliability of a
system. Finally, we have briefly looked at the techniques which make a distributed system more reliable and more
scalable. Our treatment of scalability has only scratched the surface when scaling is just straight-forward
replication of immutable machines. For the rest of the distributed systems series we will explore more complex
                                                                                                    
                                                                                                    
versions of scaling.
</p><!--l. 1--><p class="noindent" >
</p>
   <h3 class="likesectionHead"><a 
 id="x1-21000"></a>References</h3>
<!--l. 1--><p class="noindent" >
   </p><div class="thebibliography">
   <p class="bibitem" ><span class="biblabel">
 [1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xservice-based-notes"></a>R.&#x00A0;Thomas,           &#8220;Service-based           architecture,&#8221;           March           2022.                                         
<a 
href="https://csse6400.uqcloud.net/handouts/service-based.pdf" class="url" ><span 
class="ectt-1200">https://csse6400.uqcloud.net/handouts/service-based.pdf</span></a>.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfour-fallacies"></a>I.&#x00A0;V.&#x00A0;D.             Hoogen,             &#8220;Deutsch&#8217;s             fallacies,             10             years             after.&#8221;             
<a 
href="https://web.archive.org/web/20070811082651/http://java.sys-con.com/read/38665.htm" class="url" ><span 
class="ectt-1200">https://web.archive.org/web/20070811082651/http://java.sys-con.com/read/38665.htm</span></a>,
   January 2004.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xeight-fallacies"></a>P.&#x00A0;Jausovec,                    &#8220;Fallacies                    of                    distributed                    systems.&#8221;                    
<a 
href="https://blogs.oracle.com/developers/post/fallacies-of-distributed-systems" class="url" ><span 
class="ectt-1200">https://blogs.oracle.com/developers/post/fallacies-of-distributed-systems</span></a>,
   November 2020.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xtherac"></a>N.&#x00A0;Leveson and C.&#x00A0;Turner, &#8220;An investigation of the therac-25 accidents,&#8221; <span 
class="Cabin-Italic-tlf-t1-x-x-120">Computer</span>, vol.&#x00A0;26, no.&#x00A0;7,
   pp.&#x00A0;18&#8211;41, 1993.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfacebook-outage"></a>S.&#x00A0;Janardhan,            &#8220;More            details            about            the            October            4            outage.&#8221;
   <a 
href="https://engineering.fb.com/2021/10/05/networking-traffic/outage-details/" class="url" ><span 
class="ectt-1200">https://engineering.fb.com/2021/10/05/networking-traffic/outage-details/</span></a>, October
   2021.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [6]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xneumann-faults"></a>J.&#x00A0;von   Neumann,   &#8220;Probabilistic   logics   and   synthesis   of   reliable   organisms   from   unreliable
   components, automata studies,&#8221; vol.&#x00A0;34, pp.&#x00A0;43&#8211;98, 1956.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [7]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xdata-intensive"></a>M.&#x00A0;Kleppmann,  <span 
class="Cabin-Italic-tlf-t1-x-x-120">Designing  Data-Intensive  Applications:  The  big  ideas  behind  reliable,  scalable,  and</span>
   <span 
class="Cabin-Italic-tlf-t1-x-x-120">maintainable systems</span>. O&#8217;Reilly Media, Inc., March 2017.
</p>
   </div>
                                                                                                    
                                                                                                    
   <div class="footnotes"><a 
 id="x1-4002x3.1"></a>
<!--l. 67--><p class="indent" >       <span class="footnote-mark"><a 
href="#fn1x0-bk" id="fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
href="https://youtu.be/IP-rGJKSZ3s" class="url" ><span 
class="ectt-1000">https://youtu.be/IP-rGJKSZ3s</span></a></p><a 
 id="x1-6002x3.3"></a>
<!--l. 88--><p class="indent" >   <span class="footnote-mark"><a 
href="#fn2x0-bk" id="fn2x0"><sup class="textsuperscript">2</sup></a></span><a 
href="https://en.wikipedia.org/wiki/Internet_in_Australia" class="url" ><span 
class="ectt-1000">https://en.wikipedia.org/wiki/Internet_in_Australia</span></a><span 
class="Cabin-Regular-tlf-t1-">: I cannot find a supporting reference, let me know if you are</span>
<span 
class="Cabin-Regular-tlf-t1-">able.</span></p><a 
 id="x1-14002x4.2"></a>
<!--l. 187--><p class="indent" >        <span class="footnote-mark"><a 
href="#fn3x0-bk" id="fn3x0"><sup class="textsuperscript">3</sup></a></span><a 
href="https://www.youtube.com/watch?v=AaZ_RSt0KP8" class="url" ><span 
class="ectt-1000">https://www.youtube.com/watch?v=AaZ_RSt0KP8</span></a></p><a 
 id="x1-15002x4.3"></a>
<!--l. 214--><p class="indent" >   <span class="footnote-mark"><a 
href="#fn4x0-bk" id="fn4x0"><sup class="textsuperscript">4</sup></a></span><span 
class="Cabin-Regular-tlf-t1-">In a simple ideal world.</span></p>                                                                                                                                                                               </div>
 
</body></html> 

                                                                                                    


